{
    "paper": [ 
        "paper_name: the title of the paper, or leave empty if not applicable.",
        "author: the author of the paper, or leave empty if not applicable.",
        "institute: the institute of the author, or leave empty if not applicable.",
        "topic: the main research area or technique discussed in the paper, choose one from ['Information Retrieval', 'Retrieval-Augmented Generation', 'Data Selection', 'SFT'].",
        "uses_knowledge_graph: whether the method explicitly incorporates a knowledge graph as part of retrieval or reasoning choose one from ['Yes', 'No'].",
        "reasoning_depth: reasoning path length in the application scenarios of the method by the main task or benchmark, choose one from ['single-hop', 'multi-hop'].",
        "baseline: baseline model or system used for comparison in the experiments, list all the baselines mentioned in the text.",
        "retrieval_method: primary retrieval method used to obtain supporting information, choose one from ['Dense Retrieval', 'Graph-based Retrieval', 'Hybrid Retrieval', 'Sparse Retrieval', 'Web Search', 'other'].",
        "generator_model: name of the generation models used in the system (e.g., GPT-4o-mini, LLaMa2-7B-Chat), list all the baselines mentioned in the text.",
        "uses_reranker: whether the system uses an explicit reranker stage to re-score retrieved candidates before generation, choose one from ['Yes', 'No'].",
        "data_modality:  all modality of the input data handled by the system, choose one or more from ['Audio', 'Image', 'Table', 'Text', 'Code'], e.g. Text; Table; Image.",
        "evaluation_dataset: name of the dataset(s) used for evaluation in the experiments, , list all the datasets mentioned in the text.",
        "evaluation_metric: all the evaluation metric(s) reported in the experiments (e.g.,  Perplexity; BLEU; ROUGE; F1; recall@1).",
        "application_domain: primary real-world domain or application scenario targeted by the paper, choose one or more from ['General', 'Education', 'Finance', 'Sports', 'Government', 'Legal', 'Medical', 'Academic', 'Art', 'Other'], e.g. Education; Finance",
        "use_agent: whether the system explicitly adopts an agent-style architecture (e.g., tool-calling or multi-step planning) rather than a single forward pass, choose one from ['Yes', 'No'].",
        "agent_framework: main agent-style reasoning framework used by the system, choose one from ['CoT', 'ToT', 'Multi-Agent Collaboration', 'Other'], if the system does not use agent, leave it empty.",
        "performance_on_hotpotqa: the result or performance of the method on the HotpotQA benchmark. Extract any reported metric(s) such as F1, EM, accuracy, or other values. If multiple metrics are provided, separate them with `||`. If only qualitative descriptions are given (e.g., 'outperforms baseline'), extract them in short free text.",
        "performance_on_NQ: the result or performance of the method on the Natural Questions (NQ) benchmark. Extract any reported metric(s) or qualitative descriptions. Use short free-form text if no specific numbers are given.",
        "multi_turn_retrieval: whether the method explicitly supports multi-turn retrieval. Only choose one from ['Yes', 'No']."
    ]
}